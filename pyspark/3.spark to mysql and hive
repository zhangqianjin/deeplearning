1.to mysql

from pyspark.sql import SparkSession
from pyspark.sql import Row
builder = SparkSession.builder.appName(app_name)
builder.master(master)
spark = builder.enableHiveSupport().getOrCreate()
sc = spark.sparkContext
spark = SparkSession(sc)

df = spark.read.text(src_file)
rdd = df.rdd.map(lambda x:x[0].split(",")).map(lambda x:(x[0],int(x[1])+int(x[2])))
2.1
rdd1=rdd.map(lambda arr:Row('first'=arr[0],'second'=arr[1]))
df2 = spark.createDateFrame(rdd1)

2.2
from pyspark.sql.types import *
schema = StructType([StructField('first', StringType()), StructField('second', IntType())])
df3 = spark.createDataFrame(rdd, schema)
df4 = spark.read.schema(schema).load(path) #parquet格式文件



url = "jdbc:mysql://host:port/"
table = "bac"
mode = "overwrite"
properties = {'user':'abc', 'password':'123'}
df3.write.jdbc(url,table,model,properties)


2.to hive

from pyspark.sql import SparkSession
from pyspark.sql import Row
builder = SparkSession.builder.appName(app_name)
builder.master(master)
spark = builder.enableHiveSupport().getOrCreate()
sc = spark.sparkContext
spark = SparkSession(sc)

df = spark.read.text(src_file)
rdd = df.rdd.map(lambda x:x[0].split(",")).map(lambda x:(x[0],int(x[1])+int(x[2])))
2.1
rdd1=rdd.map(lambda arr:Row('first'=arr[0],'second'=arr[1]))
df2 = spark.createDateFrame(rdd1)

2.2
from pyspark.sql.types import *
schema = StructType([StructField('first', StringType()), StructField('second', IntType())])
df3 = spark.createDataFrame(rdd, schema)
df4 = spark.read.schema(schema).load(path) #parquet格式文件

//spark = builder.enableHiveSupport().getOrCreate()

spark.sql("select * from default.hive")
