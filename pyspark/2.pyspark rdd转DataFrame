1.基于rdd的反射机制转成DataFrame
text file is :
a,1,2
b,1,3
c,2,5

from pyspark.sql import SparkSession
builder = SparkSession.builder.appName(app_name)
builder.master(master)
spark = builder.enableHiveSupport().getOrCreate()
sc = spark.sparkContext
spark = SparkSession(sc)

df = spark.read.text(src_file)
rdd = df.rdd.map(lambda x:x[0].split(",")).map(lambda x:(x[0],int(x[1])+int(x[2])))
df1 = rdd.toDF()
df2 = rdd.toDF(["first","second"])

2.直接给定shcema信息
from pyspark.sql import SparkSession
from pyspark.sql import Row
builder = SparkSession.builder.appName(app_name)
builder.master(master)
spark = builder.enableHiveSupport().getOrCreate()
sc = spark.sparkContext
spark = SparkSession(sc)

df = spark.read.text(src_file)
rdd = df.rdd.map(lambda x:x[0].split(",")).map(lambda x:(x[0],int(x[1])+int(x[2])))
2.1
rdd1=rdd.map(lambda arr:Row('first'=arr[0],'second'=arr[1]))
df2 = spark.createDateFrame(rdd1)

2.2
from pyspark.sql.types import *
schema = StructType([StructField('first', StringType()), StructField('second', IntType())])
df3 = spark.createDataFrame(rdd, schema)
df4 = spark.read.schema(schema).load(path) #parquet格式文件
