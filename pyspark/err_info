
1. An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.lang.IllegalArgumentException: Required executor memory (40960), overhead (512 MB), and PySpark memory (0 MB) is above the max threshold (32768 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.

解决方法：把--executor-memory 减小
